{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Auto-PTA processor #\n",
    "######################\n",
    "#\n",
    "# Post-processing for the AutoPTA output data:\n",
    "# - Clips well data as per global dictionary (e.g. for failed gauges, changing derivative profiles)\n",
    "# - Merges clipped well data if we have many>one well mapping\n",
    "# - Checks preferred shut-in ID status and fixes ID number if necessary (e.g. update to time range, detection parameters)\n",
    "# - Uses preferred shut-in ID to perform QC scoring on each PBU/PFO based on covariance correlation to trusted data\n",
    "# - Outputs processed plots/diagnostic dfs in addition to QC score df (Spotfire does the rest)\n",
    "#\n",
    "# Last updated: 08/11/19, v1.0\n",
    "# Contact: D Robbins\n",
    "#\n",
    "#\n",
    "# Global libraries used\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "#\n",
    "###################################\n",
    "# Global variables and thresholds #\n",
    "###################################\n",
    "#\n",
    "f_unique_id_threshold = 12.0 # number of hours to be acceptable for unique ID start point to have moved\n",
    "#\n",
    "################\n",
    "# Global lists #\n",
    "################\n",
    "#\n",
    "# list of wells to remove from df (usually just use the clip/merge dictionaries below; may have older wells/bores no longer used)\n",
    "l_remove_well = ['I01'] # removed CW19 as this is the old well (CW19z), replaced by CW19x side track\n",
    "#\n",
    "#######################\n",
    "# Global dictionaries #\n",
    "#######################\n",
    "#\n",
    "# Wells to clip/merge \n",
    "#\n",
    "# Clip dictionary: well:[start,end]\n",
    "i_clipdata_clipfrom = 0\n",
    "i_clipdata_clipto = 1\n",
    "#\n",
    "d_clip_data = {\n",
    "    'I02_A':['01/01/2000 00:00:00','01/03/2008 00:00:00'],\n",
    "    'I02_B':['10/06/2012 00:00:00','01/01/2100 00:00:00'],\n",
    "    }\n",
    "#\n",
    "# Merge dictionary for clipped wells: global_well:[clipped_well_1,clipped_well_2,...,clipped_well_n]\n",
    "d_merge_wells = {\n",
    "    'I02':['I02_A','I02_B'],\n",
    "    }\n",
    "#\n",
    "# Logarithmic sampling dictionary (sets up the np.logspace sampling for reference vs non-reference covariance calculation)\n",
    "#\n",
    "# List elements for each dictionary entry:\n",
    "i_logsample_min = 0         # log10 start point on derivative plot (e.g. -1 = 10^-1 = 0.1)\n",
    "i_logsample_max = 1         # log10 end point on derivative plot (e.g. 2 = 10^2 = 100) - use shorter end points for known offset interference\n",
    "i_logsample_samples = 2     # samples per log cycle (recommend 30 - 50)\n",
    "i_logsample_method = 3      # QC score method from 2 closest neighbours; 0 = maximum (recommended), 1 = closest, 2 = distance-weighted average\n",
    "i_logsample_kh_penalty = 4  # apply a (log kh ratio / n) penalty to QC score; 0.0 = no penalty, recommend 1.0-2.0\n",
    "#\n",
    "d_logsample_params = {\n",
    "    #\n",
    "    'I01':[-1,3,40,0,1.25],\n",
    "    'I02':[-1,3,40,0,1.25],\n",
    "    }\n",
    "#\n",
    "####################\n",
    "# Global functions #\n",
    "####################\n",
    "#\n",
    "# Function to find the nearest two neighbours to a value (you don't need to do anything to this)\n",
    "def nearest_two_neighbours(value,lst):\n",
    "    #\n",
    "    my_two_values = []\n",
    "    #\n",
    "    # build list of distances\n",
    "    my_distance = [(i-value) for i in lst]\n",
    "    my_euclid_distance = [abs(i-value) for i in lst]\n",
    "    #\n",
    "    # spit out first value (remember to use the sign not absolute Euclidean!)\n",
    "    my_two_values.append(my_distance[my_euclid_distance.index(min(my_euclid_distance))]+value)\n",
    "    #\n",
    "    # remove first value from two distance lists and any others on the same side\n",
    "    if (value - (my_distance[my_euclid_distance.index(min(my_euclid_distance))]+value)) < 0: # remove greater than\n",
    "        new_lst = [i for i in lst if i < my_distance[my_euclid_distance.index(min(my_euclid_distance))]+value]\n",
    "    else:\n",
    "        new_lst = [i for i in lst if i > my_distance[my_euclid_distance.index(min(my_euclid_distance))]+value]\n",
    "    #\n",
    "    # re-build list of distances\n",
    "    my_distance = [(i-value) for i in new_lst]\n",
    "    my_euclid_distance = [abs(i-value) for i in new_lst]\n",
    "    #\n",
    "    # return the second value (use try-except if we only have one side)\n",
    "    try:\n",
    "        my_two_values.append(my_distance[my_euclid_distance.index(min(my_euclid_distance))]+value)\n",
    "    except:\n",
    "        pass\n",
    "    #\n",
    "    # return two nearest neighbours\n",
    "    return my_two_values\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_plots(plots):\n",
    "    #\n",
    "    # first point of processing for the PLOTS data table (derivative/dP data):\n",
    "    # - perform datetime clips on wells as pre requested\n",
    "    # - merge multiple pseudo-wells\n",
    "    #\n",
    "    df = plots.copy(deep=True)\n",
    "    #\n",
    "    # update datetime column to make it TZ-aware\n",
    "    df['start_time'] = pd.to_datetime(df['start_time'],utc=True)\n",
    "    #\n",
    "    # build list of wells to leave alone, refer to global dictionaries\n",
    "    my_keep_wells = []\n",
    "    for well in df['well_id'].unique():\n",
    "        if well not in l_remove_well and well not in d_clip_data:\n",
    "            my_keep_wells.append(well)\n",
    "    #\n",
    "    # split the df into wells to clip and wells to leave alone\n",
    "    df_toclip = df[df['well_id'].isin(d_clip_data)]\n",
    "    df = df[df['well_id'].isin(my_keep_wells)]\n",
    "    #\n",
    "    # apply the clipping requested in the global dictionary\n",
    "    df_clip = pd.DataFrame(columns=df.columns)\n",
    "    #\n",
    "    for well in df_toclip['well_id'].unique():\n",
    "        #\n",
    "        # pull clips from the dictionary\n",
    "        clip_from = pd.to_datetime(d_clip_data[well][i_clipdata_clipfrom],utc=True,dayfirst=True)\n",
    "        clip_to = pd.to_datetime(d_clip_data[well][i_clipdata_clipto],utc=True,dayfirst=True)\n",
    "        #\n",
    "        # build df of clipped data for this well\n",
    "        df_well = df_toclip[df_toclip['well_id']==well]\n",
    "        #\n",
    "        # perform clip\n",
    "        df_well = df_well[(df_well['start_time']>=clip_from)&(df_well['start_time']<=clip_to)]\n",
    "        #\n",
    "        # add to df of clipped data\n",
    "        df_clip = pd.concat([df_clip,df_well],ignore_index=True)\n",
    "        #\n",
    "    #\n",
    "    # now merge the wells in the clipped df\n",
    "    df_merge = pd.DataFrame(columns=df.columns)\n",
    "    #\n",
    "    for well in d_merge_wells:\n",
    "        #\n",
    "        # pull list of labels to merge\n",
    "        l_to_merge = d_merge_wells[well]\n",
    "        #\n",
    "        # verify that merge wells exist in df\n",
    "        for merge_well in l_to_merge:\n",
    "            if len(df_clip[df_clip['well_id']==merge_well]) == 0:\n",
    "                print('Warning: well ',merge_well,\" doesn't exist in clip dataframe (d_merge_cells,d_clip_data)\")\n",
    "        #\n",
    "        # rename the column names in the clip columns to be the new unified well\n",
    "        df_well = df_clip[df_clip['well_id'].isin(l_to_merge)].copy(deep=True)\n",
    "        df_well.loc[:,'well_id'] = [well]*len(df_well)\n",
    "        df_well = df_well.sort_values(by='start_time')\n",
    "        #\n",
    "        # add to df of merged data\n",
    "        df_merge = pd.concat([df_merge,df_well],ignore_index=True)\n",
    "        #\n",
    "    #\n",
    "    # merge df with the master df\n",
    "    df = pd.concat([df,df_merge],ignore_index=True)\n",
    "    #\n",
    "    # replace blanks with NaN so PySpark doesn't fall over\n",
    "    df.replace('',np.nan,inplace=True)\n",
    "    #\n",
    "    print('Successfully clipped/merged plots data table.')\n",
    "    #\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_PTA(pta):\n",
    "    #\n",
    "    # first point of processing for the PLOTS data table (derivative/dP data):\n",
    "    # - perform datetime clips on wells as pre requested\n",
    "    # - merge multiple pseudo-wells\n",
    "    #\n",
    "    df = pta.copy(deep=True)\n",
    "    #\n",
    "    # update datetime column to make it TZ-aware\n",
    "    df['start_time'] = pd.to_datetime(df['start_time'],utc=True)\n",
    "    #\n",
    "    # build list of wells to leave alone, refer to global dictionaries\n",
    "    my_keep_wells = []\n",
    "    for well in df['well_id'].unique():\n",
    "        if well not in l_remove_well and well not in d_clip_data:\n",
    "            my_keep_wells.append(well)\n",
    "    #\n",
    "    # split the df into wells to clip and wells to leave alone\n",
    "    df_toclip = df[df['well_id'].isin(d_clip_data)]\n",
    "    df = df[df['well_id'].isin(my_keep_wells)]\n",
    "    #\n",
    "    # apply the clipping requested in the global dictionary\n",
    "    df_clip = pd.DataFrame(columns=df.columns)\n",
    "    #\n",
    "    for well in df_toclip['well_id'].unique():\n",
    "        #\n",
    "        # pull clips from the dictionary\n",
    "        clip_from = pd.to_datetime(d_clip_data[well][i_clipdata_clipfrom],utc=True,dayfirst=True)\n",
    "        clip_to = pd.to_datetime(d_clip_data[well][i_clipdata_clipto],utc=True,dayfirst=True)\n",
    "        #\n",
    "        # build df of clipped data for this well\n",
    "        df_well = df_toclip[df_toclip['well_id']==well]\n",
    "        #\n",
    "        # perform clip\n",
    "        df_well = df_well[(df_well['start_time']>=clip_from)&(df_well['start_time']<=clip_to)]\n",
    "        #\n",
    "        # add to df of clipped data\n",
    "        df_clip = pd.concat([df_clip,df_well],ignore_index=True)\n",
    "        #\n",
    "    #\n",
    "    # now merge the wells in the clipped df\n",
    "    df_merge = pd.DataFrame(columns=df.columns)\n",
    "    #\n",
    "    for well in d_merge_wells:\n",
    "        #\n",
    "        # pull list of labels to merge\n",
    "        l_to_merge = d_merge_wells[well]\n",
    "        #\n",
    "        # verify that merge wells exist in df\n",
    "        for merge_well in l_to_merge:\n",
    "            if len(df_clip[df_clip['well_id']==merge_well]) == 0:\n",
    "                print('Warning: well ',merge_well,\" doesn't exist in clip dataframe (d_merge_cells,d_clip_data)\")\n",
    "        #\n",
    "        # rename the column names in the clip columns to be the new unified well\n",
    "        df_well = df_clip[df_clip['well_id'].isin(l_to_merge)].copy(deep=True)\n",
    "        df_well.loc[:,'well_id'] = [well]*len(df_well)\n",
    "        df_well = df_well.sort_values(by='start_time')\n",
    "        #\n",
    "        # add to df of merged data\n",
    "        df_merge = pd.concat([df_merge,df_well],ignore_index=True)\n",
    "        #\n",
    "    #\n",
    "    # merge df with the master df\n",
    "    df = pd.concat([df,df_merge],ignore_index=True)\n",
    "    #\n",
    "    # replace blanks with NaN so PySpark doesn't fall over\n",
    "    df.replace('',np.nan,inplace=True)\n",
    "    #\n",
    "    print('Successfully clipped/merged PTA data table.')\n",
    "    #\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_shutin_ids(pta):\n",
    "    #\n",
    "    # reset shut-in IDs and build df of the mapping performed\n",
    "    #\n",
    "    # first, we build a list of the shut-in IDs from the original PTA df\n",
    "    # (there's probably a more efficient way to do this, but I did all the mods at the same time\n",
    "    #    in my Jupyter notebook but I can't output many dfs in one Code Workbook transform ...)\n",
    "    #\n",
    "    # original PTA df\n",
    "    df = pta.copy(deep=True)\n",
    "    #\n",
    "    # update datetime column to make it TZ-aware\n",
    "    df['start_time'] = pd.to_datetime(df['start_time'],utc=True)\n",
    "    #\n",
    "    # split the df into wells to clip and wells to leave alone\n",
    "    df_toclip = df[df['well_id'].isin(d_clip_data)]\n",
    "    #\n",
    "    # apply the clipping requested in the global dictionary\n",
    "    df_clip = pd.DataFrame(columns=df.columns)\n",
    "    #\n",
    "    for well in df_toclip['well_id'].unique():\n",
    "        #\n",
    "        # pull clips from the dictionary\n",
    "        clip_from = pd.to_datetime(d_clip_data[well][i_clipdata_clipfrom],utc=True,dayfirst=True)\n",
    "        clip_to = pd.to_datetime(d_clip_data[well][i_clipdata_clipto],utc=True,dayfirst=True)\n",
    "        #\n",
    "        # build df of clipped data for this well\n",
    "        df_well = df_toclip[df_toclip['well_id']==well]\n",
    "        #\n",
    "        # perform clip\n",
    "        df_well = df_well[(df_well['start_time']>=clip_from)&(df_well['start_time']<=clip_to)]\n",
    "        #\n",
    "        # add to df of clipped data\n",
    "        df_clip = pd.concat([df_clip,df_well],ignore_index=True)\n",
    "        #\n",
    "    #\n",
    "    # with the clipped df, build a list (of lists) of well - old shut-in ID - new shut-in ID\n",
    "    #\n",
    "    l_shutin_convert = []\n",
    "    #\n",
    "    for well in d_merge_wells:\n",
    "        #\n",
    "        # build list of clipped unique shut-in IDs and re-assign\n",
    "        l_well_si_ids = df_clip[df_clip['well_id'].isin(d_merge_wells[well])][['start_time','shutin_id']].sort_values(by='start_time')\n",
    "        l_well_si_ids = l_well_si_ids['shutin_id'].tolist()\n",
    "        #\n",
    "        # loop over list and generate mapping of old>new shut-in ID\n",
    "        for i_si,shut_in in enumerate(l_well_si_ids):\n",
    "            new_shutin_id = str('GL_')+str(well)+str('_')+str(i_si+1)\n",
    "            l_shutin_convert.append([well,shut_in,new_shutin_id])\n",
    "        #\n",
    "    #\n",
    "    # write list-of-lists to df\n",
    "    df = pd.DataFrame(l_shutin_convert,columns=['well_id','old_shutin_id','new_shutin_id'])\n",
    "    #\n",
    "    print('Built merged/spliced well shut-in ID conversion OK.')\n",
    "    #\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_PTA_ids(process_PTA, reset_shutin_ids):\n",
    "    #\n",
    "    # take the processed (clipped/merged) plots df and assign new shut-in IDs\n",
    "    # - this only happens on clipped/merged wells, so just loop over these\n",
    "    #\n",
    "    # copy the two dataframes\n",
    "    df = process_PTA.copy(deep=True)\n",
    "    df_shutins = reset_shutin_ids.copy(deep=True)\n",
    "    #\n",
    "    for well in d_merge_wells:\n",
    "        #\n",
    "        # generate shut-in df, use the old shut-in ID as the index so we can df.loc on this\n",
    "        df_shutin_well = df_shutins[df_shutins['well_id']==well]\n",
    "        #\n",
    "        # build dictionary from the shut-in IDs\n",
    "        d_well_shutins = {}\n",
    "        for i_df,row_df in df_shutin_well.iterrows():\n",
    "            d_well_shutins[row_df['old_shutin_id']] = row_df['new_shutin_id']\n",
    "        #\n",
    "        # loop over shut-ins and re-assign shut-in ID\n",
    "        for shutin_id in df[df['well_id']==well]['shutin_id'].unique():\n",
    "            try:\n",
    "                df.loc[df['shutin_id']==shutin_id,'shutin_id'] = d_well_shutins[shutin_id]\n",
    "            except:\n",
    "                pass\n",
    "    #\n",
    "    print('Successfully re-assigned shut-in IDs in the PTA data table.')\n",
    "    #\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_plots_IDs(process_plots, reset_shutin_ids, reset_PTA_ids):\n",
    "    #\n",
    "    # take the processed (clipped/merged) plots df and assign new shut-in IDs\n",
    "    # - this only happens on clipped/merged wells, so just loop over these\n",
    "    #\n",
    "    # for the plots df, we also unify shut-in IDs on the PTA df to ensure consistency\n",
    "    #\n",
    "    # copy the two dataframes\n",
    "    df = process_plots.copy(deep=True)\n",
    "    df_shutins = reset_shutin_ids.copy(deep=True)\n",
    "    df_PTA = reset_PTA_ids.copy(deep=True)\n",
    "    #\n",
    "    for well in d_merge_wells:\n",
    "        #\n",
    "        # generate shut-in df, use the old shut-in ID as the index so we can df.loc on this\n",
    "        df_shutin_well = df_shutins[df_shutins['well_id']==well]\n",
    "        #\n",
    "        # build dictionary from the shut-in IDs\n",
    "        d_well_shutins = {}\n",
    "        for i_df,row_df in df_shutin_well.iterrows():\n",
    "            d_well_shutins[row_df['old_shutin_id']] = row_df['new_shutin_id']\n",
    "        #\n",
    "        # loop over shut-ins and re-assign shut-in ID\n",
    "        for shutin_id in df[df['well_id']==well]['shutin_id'].unique():\n",
    "            try:\n",
    "                df.loc[df['shutin_id']==shutin_id,'shutin_id'] = d_well_shutins[shutin_id]\n",
    "            except:\n",
    "                pass\n",
    "    #\n",
    "    # clip the plots df on the basis of shut-in IDs identified in the main PTA df\n",
    "    #\n",
    "    i_len_df = len(df)                                          # remember the length pre-clip\n",
    "    df = df[df['shutin_id'].isin(df_PTA['shutin_id'].unique())] # perform the clip\n",
    "    #\n",
    "    print('Successfully re-assigned shut-in IDs in the plots data table.')\n",
    "    print('Plots dataframe size is: %.2f pc of original after unifying shut-in IDs.')%((len(df)/float(i_len_df))*100.0)\n",
    "    #\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_reference_shutin_ids(reset_PTA_ids, preferred_shutin_ids):\n",
    "    #\n",
    "    # Loop through the preferred shut-ins selected and determine if they are still the same\n",
    "    # -> if ID has changed, update the unique shut-in ID\n",
    "    #\n",
    "    # Initialise loop counters\n",
    "    i_matched = 0\n",
    "    i_different_ID = 0\n",
    "    i_slight_shift = 0\n",
    "    i_slight_shift_newID = 0\n",
    "    i_failed = 0\n",
    "    my_new_preferred_list = []\n",
    "    #\n",
    "    print('Processing unique ID data ...')\n",
    "    #\n",
    "    # deep copy input dataframes\n",
    "    df_PTA = reset_PTA_ids.copy(deep=True)\n",
    "    df_preferred_shutins = preferred_shutin_ids.copy(deep=True)\n",
    "    #\n",
    "    # set up date-time format on preferred shut-ins df column\n",
    "    df_preferred_shutins['pbu_pfo_date'] = pd.to_datetime(df_preferred_shutins['pbu_pfo_date'],utc=True,dayfirst=True)\n",
    "    df_PTA['start_time'] = pd.to_datetime(df_PTA['start_time'],utc=True)\n",
    "    #\n",
    "    # loop over preferred shut-in ID df\n",
    "    for i,row in df_preferred_shutins.iterrows():\n",
    "        #\n",
    "        # initialise shut-in ID status\n",
    "        i_check_QC = 0 # if this becomes 1, we believe shut-in ID hasn't been changed or we have found a match\n",
    "        #\n",
    "        # grab row information\n",
    "        shutin_well = row['well_id']\n",
    "        shutin_date = row['pbu_pfo_date']\n",
    "        old_shutin_id = row['shutin_id']\n",
    "        #\n",
    "        # look up this shut in ID on the new data (will create empty series if no slice found)\n",
    "        s_new_data = df_PTA[df_PTA['shutin_id']==old_shutin_id]\n",
    "        t_new_data = df_PTA[(pd.to_datetime(df_PTA['start_time'])==shutin_date) & (df_PTA['well_id']==shutin_well)]\n",
    "        #   \n",
    "        # if we find a PBU with exactly the same date-time, take this point as the new unique ID\n",
    "        if len(t_new_data) == 1:\n",
    "            i_check_QC = 1\n",
    "            i_store_ID = t_new_data['shutin_id'].iloc[0]\n",
    "            date_store = shutin_date\n",
    "            if i_store_ID == old_shutin_id:\n",
    "                i_matched += 1 # counter of exactly matched unique IDs\n",
    "            else:\n",
    "                i_different_ID += 1 # counter of changed unique IDs at same date-time\n",
    "        else:\n",
    "            # if it's not exactly matched, firstly check unique ID match if there is one\n",
    "            if len(s_new_data) == 1: # shouldn't be more than 1 row, but just in case don't use >0 logic\n",
    "                #\n",
    "                # check the time here\n",
    "                new_ID_time = s_new_data['start_time'].iloc[0]\n",
    "                delta_t = abs(new_ID_time - shutin_date)/pd.Timedelta(1,'h')\n",
    "                #\n",
    "                # if the time delta is \"close enough\" then use this shutin ID (threshold value is set in global code)\n",
    "                if delta_t < f_unique_id_threshold:\n",
    "                    i_check_QC = 1\n",
    "                    i_store_ID = s_new_data['shutin_id'].iloc[0]\n",
    "                    i_slight_shift += 1\n",
    "                    date_store = new_ID_time\n",
    "            #\n",
    "            # if no (or multiple?!) unique ID match(es), look to find a PBU/PFO within a slight time offset\n",
    "            # also check here if we find that the unique ID doesn't match the other one\n",
    "            if i_check_QC != 1:\n",
    "                #\n",
    "                # grab new df using well and shut-in data += a timedelta offset (use 12 hrs as a guess)\n",
    "                t_new_data = df_PTA[(df_PTA['well_id']==shutin_well)&(df_PTA['start_time']<(shutin_date+pd.Timedelta(f_unique_id_threshold,'h')))&(df_PTA['start_time']>(shutin_date-pd.Timedelta(f_unique_id_threshold,'h')))]\n",
    "                #\n",
    "                # if only one PBU/PFO found within timedelta window, use this as the updated reference shut-in ID\n",
    "                if len(t_new_data) == 1:\n",
    "                    i_check_QC = 1\n",
    "                    i_store_ID = t_new_data['shutin_id'].iloc[0]\n",
    "                    i_slight_shift_newID += 1\n",
    "                    date_store = t_new_data['start_time'].iloc[0]\n",
    "                # if more than one, find the closest\n",
    "                elif len(t_new_data) > 1:\n",
    "                    t_delta = 100000.0\n",
    "                    date_store = t_new_data['start_time'].iloc[0] # placeholder just in case, will be overwritten in loop\n",
    "                    # loop over each shut-in in subset timedelta window and find the closest to the original shut-in time\n",
    "                    for i,row in t_new_data.iterrows():\n",
    "                        delta_t = abs(row['start_time'] - shutin_date)/pd.Timedelta(1,'h')\n",
    "                        if delta_t < t_delta:\n",
    "                            t_delta = delta_t\n",
    "                            date_store = row['start_time']\n",
    "                            my_shutin_ID_loop = row['shutin_id']  \n",
    "                    i_check_QC = 1\n",
    "                    i_store_ID = my_shutin_ID_loop\n",
    "                    i_slight_shift_newID += 1\n",
    "                else:\n",
    "                    # report that we couldn't find a PBU/PFO close to the data\n",
    "                    i_check_QC = 2\n",
    "                    i_failed += 1\n",
    "                    print('Failed to match old ID: ',old_shutin_id)\n",
    "                #\n",
    "            #\n",
    "        #\n",
    "        # build the new preferred shut-in df\n",
    "        if i_check_QC == 1:\n",
    "            my_new_preferred_list.append([shutin_well,i_store_ID,'Y',date_store])\n",
    "        #\n",
    "        # print some debug information\n",
    "        if i_check_QC == 0:\n",
    "            print('WARNING: Unique ID ',old_shutin_id,' failed to find a match and unexpectedly passed through all the logic ... fail')\n",
    "    #\n",
    "    # write the list of preferred shut-ins to new df\n",
    "    df_preferred_shutins_new = pd.DataFrame(my_new_preferred_list,columns=['well_id','shutin_id','shutin_preferred','pbu_pfo_date'])\n",
    "    #\n",
    "    # print some summary statistics\n",
    "    print('')\n",
    "    print(str('Number of unique PFOs in old file: '+str(len(df_preferred_shutins))))\n",
    "    print(str('Number of unique PFOs matched: '+str(i_matched+i_different_ID+i_slight_shift+i_slight_shift_newID)))\n",
    "    print(str('... exactly matched on date-time, same ID: '+str(i_matched)))\n",
    "    print(str('... exactly matched on date-time, different ID: '+str(i_different_ID)))\n",
    "    print(str('... slightly shifted, same ID: '+str(i_slight_shift)))\n",
    "    print(str('... slightly shifted, different ID: '+str(i_slight_shift_newID)))\n",
    "    print(str('Failed to match: '+str(i_failed)))\n",
    "    #\n",
    "    # return df\n",
    "    return df_preferred_shutins_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qc_analysis(update_reference_shutin_ids, reset_plots_IDs):\n",
    "    #\n",
    "    # Perform QC analysis of all shut-in events\n",
    "    # -> computes Spearman rank coefficient between shut-in and reference shut-in\n",
    "    # -> computes penalty coefficient for kh ratio offset\n",
    "    #\n",
    "    # this is disgustingly inefficient but works ... \n",
    "    #\n",
    "    # make deep copy of each df\n",
    "    df_plots = reset_plots_IDs.copy(deep=True)\n",
    "    df_preferred_shutins_new = update_reference_shutin_ids.copy(deep=True)\n",
    "    #\n",
    "    # firstly, set up a new \"unique_id\" column that we'll use to derive closest neighbour in a fast manner\n",
    "    df_plots['id_number'] = df_plots['shutin_id'].str.split(\"_\").str[-1]\n",
    "    df_preferred_shutins_new['id_number'] = df_preferred_shutins_new['shutin_id'].str.split(\"_\").str[-1]\n",
    "    #\n",
    "    # convert split numbers to integers\n",
    "    df_plots = df_plots.astype({'id_number':'int64'})\n",
    "    df_preferred_shutins_new = df_preferred_shutins_new.astype({'id_number':'int64'})\n",
    "    #\n",
    "    # set up QC lists\n",
    "    my_QC_id = []\n",
    "    my_QC_score = []\n",
    "    my_kh_ratio = []\n",
    "    #\n",
    "    # get number of wells to generate an excruciating counter\n",
    "    i_no_wells = len(df_plots['well_id'].unique())\n",
    "    #\n",
    "    # loop over all wells and perform QC\n",
    "    i_wells_QC = 0 # counter\n",
    "    #\n",
    "    for well in df_plots['well_id'].unique():\n",
    "        #\n",
    "        # check that this well has preferred shut-in ids\n",
    "        df_well_pref = df_preferred_shutins_new[df_preferred_shutins_new['well_id']==well]\n",
    "        #\n",
    "        # get the df for this well\n",
    "        df_well = df_plots[df_plots['well_id']==well]\n",
    "        #\n",
    "        # get the sampling parameters (from global dictionary)\n",
    "        try:\n",
    "            l_logsample_params = d_logsample_params[well]\n",
    "        except: # fall back option is CP01 parameters, as CP01 is the best well\n",
    "            l_logsample_params = d_logsample_params['CP01']\n",
    "        #\n",
    "        # if the well does have preferred shut-ins identified, loop over all shut-ins and do the QC scoring\n",
    "        if len(df_well_pref) > 0:\n",
    "            #\n",
    "            # list of reference PBU/PFOs\n",
    "            l_ref_shutin = df_well_pref['shutin_id'].tolist()\n",
    "            #\n",
    "            # loop over each PBU/PFO (I call the variable PFO because I'm on team water injection)\n",
    "            for PFO in df_well['shutin_id'].unique():\n",
    "                #\n",
    "                # if shut-in is a preferred one, set QC = 2 and move on\n",
    "                if PFO in l_ref_shutin:\n",
    "                    #\n",
    "                    # set up QC = 2 if well is a reference shut-in\n",
    "                    my_QC = 2.0\n",
    "                    #\n",
    "                    my_QC_id.append(PFO)\n",
    "                    my_QC_score.append(my_QC)\n",
    "                    my_kh_ratio.append(1.0)\n",
    "                    #\n",
    "                #\n",
    "                # if shut-in isn't a preferred one, perform QC scoring\n",
    "                else:\n",
    "                    #\n",
    "                    # get PFO dataframe (only use derivative plot, not dP, to do the QC as dP position is a function of skin)\n",
    "                    df_PFO = df_well[(df_well['shutin_id']==PFO)&(df_well['series']=='Derv')]\n",
    "                    #\n",
    "                    # find closest reference shut-in; may only have one if all on same side\n",
    "                    # (nearest two neighbours is a global function)\n",
    "                    n2n = nearest_two_neighbours(df_PFO['id_number'].iloc[0],df_well_pref['id_number'].tolist())\n",
    "                    #\n",
    "                    # loop over the two closest reference PFOs and calculate the Spearman rank coefficient\n",
    "                    #\n",
    "                    # (lists to build for spearman rank coefficient, timedelta to ref PFO, kh/stabilisation observed)\n",
    "                    ref_S_list = []\n",
    "                    ref_S_dt = []\n",
    "                    ref_kh_list = []\n",
    "                    #\n",
    "                    for ref_pfo in n2n:\n",
    "                        #\n",
    "                        # pull the reference PFO (df_PFO_ref) to compare the shut-in (df_PFO) against\n",
    "                        df_PFO_ref = df_well[(df_well['shutin_id']==('GL_'+well+'_'+str(ref_pfo)))&(df_well['series']=='Derv')]\n",
    "                        #\n",
    "                        # proceed if dfs have been pulled properly\n",
    "                        if (len(df_PFO) > 0) and (len(df_PFO_ref) > 0):\n",
    "                            #\n",
    "                            # build the log sampling grid (i_logsample_X is the integer reference to the list supplied in the d_logsample_params dictionary)\n",
    "                            sample_grid = np.logspace(l_logsample_params[i_logsample_min],l_logsample_params[i_logsample_max],num=int(l_logsample_params[i_logsample_samples]*(l_logsample_params[i_logsample_max]-l_logsample_params[i_logsample_min])))\n",
    "                            #\n",
    "                            # reset derivative lists\n",
    "                            ave_value_compare = []\n",
    "                            ave_value_ref = []\n",
    "                            #\n",
    "                            # build the logarithmic grid sampled derivatives\n",
    "                            for i in range(0,len(sample_grid)-1):\n",
    "                                #\n",
    "                                filtered_data = df_PFO[(df_PFO['delta_time']>=sample_grid[i])&(df_PFO['delta_time']<sample_grid[i+1])]['value']\n",
    "                                filtered_data_ref = df_PFO_ref[(df_PFO_ref['delta_time']>=sample_grid[i])&(df_PFO_ref['delta_time']<sample_grid[i+1])]['value']\n",
    "                                #\n",
    "                                # incorporate this sampled bucket ONLY if both PFOs have values\n",
    "                                if (len(filtered_data) > 0) & (len(filtered_data_ref) > 0):\n",
    "                                    ave_value_compare.append(np.mean(filtered_data))\n",
    "                                    ave_value_ref.append(np.mean(filtered_data_ref))\n",
    "                                #\n",
    "                            #\n",
    "                            # generate the covariance coefficient if more than one value in list, otherwise set value to zero (no QC)\n",
    "                            if (len(ave_value_compare) > 1) & (len(ave_value_ref) > 1):\n",
    "                                ref_S_list.append(spearmanr(ave_value_compare,ave_value_ref)[0])\n",
    "                            else:\n",
    "                                ref_S_list.append(0.0)\n",
    "                            #\n",
    "                            # append the delta t (for weighting between two reference PFOs if applicable)\n",
    "                            ref_S_dt.append(abs((df_PFO_ref['start_time'].iloc[0] - df_PFO['start_time'].iloc[0]).total_seconds()))\n",
    "                            #\n",
    "                            # append the kh ratio of the two derivative plots\n",
    "                            ref_kh_list.append(df_PFO_ref['stabilisation_value'].iloc[0]/df_PFO['stabilisation_value'].iloc[0])\n",
    "                        #\n",
    "                        # if no data have been pulled for this reference PFO, append a fat zero ...\n",
    "                        else:\n",
    "                            ref_S_list.append(0.0)\n",
    "                            ref_S_dt.append(1000000000)\n",
    "                            ref_kh_list.append(1.0)\n",
    "                    #\n",
    "                    # generate the QC score averaged/sampled across the two nearest reference PFOs\n",
    "                    #\n",
    "                    # options specified in the global dictionary:\n",
    "                    # 0 = maximum (recommended), 1 = closest, 2 = distance-weighted average\n",
    "                    #\n",
    "                    # option 1 - use the maximum QC score (recommended)\n",
    "                    if l_logsample_params[i_logsample_method] == 0:\n",
    "                        my_QC = max(ref_S_list)\n",
    "                        my_kh = ref_kh_list[ref_S_list.index(max(ref_S_list))]\n",
    "                    #\n",
    "                    # option 2 - use the closest QC score\n",
    "                    elif l_logsample_params[i_logsample_method] == 1:\n",
    "                        if len(ref_S_list) > 1:\n",
    "                            my_QC = 0.0\n",
    "                            my_kh = 1.0\n",
    "                            loop_dt = 0.0\n",
    "                            for S,dt,kh in zip(ref_S_list,ref_S_dt,ref_kh_list):\n",
    "                                if dt > loop_dt:\n",
    "                                    loop_dt = dt\n",
    "                                    my_QC = S\n",
    "                                    my_kh = kh\n",
    "                        else:\n",
    "                            my_QC = ref_S_list[0]\n",
    "                            my_kh = ref_kh_list[0]\n",
    "                    #\n",
    "                    # option 3 - weight by (time) distance if more than one value\n",
    "                    else: \n",
    "                        if len(ref_S_list) > 1:\n",
    "                            my_QC = 0.0\n",
    "                            my_kh = 0.0\n",
    "                            for S,dt,kh in zip(ref_S_list,ref_S_dt,ref_kh_list):\n",
    "                                my_QC += S*(1.0-(dt/sum(ref_S_dt)))\n",
    "                                my_kh += kh*(1.0-(dt/sum(ref_S_dt)))\n",
    "                        else:\n",
    "                            my_QC = ref_S_list[0]\n",
    "                            my_kh = ref_kh_list[0]\n",
    "                    #\n",
    "                    # finally, apply kh penalty to QC score\n",
    "                    if l_logsample_params[i_logsample_kh_penalty] > 0.0:\n",
    "                        try:\n",
    "                            # apply penalty to QC score\n",
    "                            my_QC -= abs(np.log10(my_kh))/l_logsample_params[i_logsample_kh_penalty]\n",
    "                            # apply thresholds to ensure QC bound between [-1.0, 1.0]\n",
    "                            my_QC = min(1.0,max(-1.0,my_QC))\n",
    "                            #\n",
    "                        except: # no kh score / NaN error\n",
    "                            my_QC = -0.99 # rather than -1.0 so should enable us to see them in SPF ...\n",
    "                    #\n",
    "                    # write S scores and kh ratio to list\n",
    "                    my_QC_id.append(PFO)\n",
    "                    my_QC_score.append(my_QC)\n",
    "                    my_kh_ratio.append(my_kh)\n",
    "                #\n",
    "            # end of loop over each PFO in well\n",
    "            print('Well '+well+' QC performed successfully.')\n",
    "        #\n",
    "        # for all wells with NO reference derivatives, append a zero score by default\n",
    "        else:\n",
    "            for PFO in df_well['shutin_id'].unique():\n",
    "                my_QC_id.append(PFO)\n",
    "                my_QC_score.append(0.0)\n",
    "                my_kh_ratio.append(1.0)\n",
    "            print('No QC performed for well '+well+' - no reference derivatives found.')\n",
    "        #\n",
    "        i_wells_QC += 1\n",
    "        #\n",
    "        print(str('QC completion status: ')+str(i_wells_QC)+str('/')+str(i_no_wells))\n",
    "    #\n",
    "    # generate QC df from the lists\n",
    "    df_QC = pd.DataFrame()\n",
    "    df_QC.insert(len(df_QC.columns),'shutin_id',my_QC_id)\n",
    "    df_QC.insert(len(df_QC.columns),'qc_score',my_QC_score)\n",
    "    df_QC.insert(len(df_QC.columns),'kh_ratio',my_kh_ratio)\n",
    "    #\n",
    "    # return the df\n",
    "    return df_QC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
